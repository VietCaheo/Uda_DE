{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tempo remove during local test\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tempo remove during local test\n",
    "\n",
    "# ARN_IAM_ROLE = config.get('IAM_ROLE', 'ARN')\n",
    "\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# os.environ['AWS_ACCESS_KEY_ID']=config.get('IAM_ACCESS_KEY', 'AWS_ACCESS_KEY_ID')\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']=config.get('IAM_ACCESS_KEY', 'AWS_SECRET_ACCESS_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    # song_data = input_data\n",
    "    \n",
    "    # read song data file\n",
    "    #df = spark.read.json('data/song-data.zip')\n",
    "    df = spark.read.json(input_data)\n",
    "    \n",
    "    #[!] with json file, automatic infer schema when read.json file. But for reading csv file might be different\n",
    "    print(\"to see the original df read from json song data \\n\")\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    # see above needed functions from spark.sql.functions\n",
    "    \n",
    "    # extract dimensional table `songs_table`: df.select(), return a new dataframe from original one `df`\n",
    "    print(\"extract from root schema to be  dimensional table songs_table \\n\")\n",
    "    songs_table = df.select('song_id', \\\n",
    "                            'title', \\\n",
    "                            'artist_id', \\\n",
    "                            'year', \\\n",
    "                            'duration')\n",
    "    songs_table.printSchema()\n",
    "    songs_table.show(5)\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    \n",
    "    # save df songs_table as Parquet files, maintaining the schema information\n",
    "    songs_table.write \\\n",
    "               .partitionBy(\"year\", \"artist_id\") \\\n",
    "               .mode('overwrite') \\\n",
    "               .parquet(\"songs.parquet\")\n",
    "    \n",
    "    # This just for testing after saved the parquet files from df\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # read the parquet files created above as a partitioning df\n",
    "    parquetSongsFile = spark.read.parquet(\"songs.parquet\")\n",
    "    \n",
    "    # creat tempo view from parquet df above\n",
    "    parquetSongsFile.createOrReplaceTempView(\"parquetSongsFile\")\n",
    "    \n",
    "    # to see the parquet view\n",
    "    print(\"to see songs parque view, partitioned by artist_id and year \\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM parquetSongsFile\n",
    "            ORDER by year\n",
    "            limit 5\n",
    "            \"\"\").show()\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # extract columns to create artists table\n",
    "    print(\"<###################################################################### \\n\")\n",
    "    print(\"extract from root schema to be dimensional table artists_table \\n\")\n",
    "    artists_table = df.select('artist_id', \\\n",
    "                              col('artist_name').alias('name'), \\\n",
    "                              col('artist_location').alias('location'), \\\n",
    "                              col('artist_latitude').alias('latitude'), \\\n",
    "                              col('artist_longitude').alias('longitude'))\n",
    "    artists_table.printSchema()\n",
    "    artists_table.show(5)\n",
    "    \n",
    "    # artists_table = \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    # partition by name and artist_id\n",
    "    artists_table.write \\\n",
    "                 .partitionBy(\"name\", \"artist_id\") \\\n",
    "                 .mode('overwrite') \\\n",
    "                 .parquet(\"artists.parquet\")\n",
    "    \n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # For test paquest df created above    \n",
    "    # read back parquet files after saving above\n",
    "    parquetArtistFile = spark.read.parquet(\"artists.parquet\")\n",
    "    \n",
    "    # creat tempo parquet view\n",
    "    print(\"######################################################################### \\n \")\n",
    "    parquetArtistFile.createOrReplaceTempView(\"parquetArtistFile\")\n",
    "    \n",
    "    print(\"to see artists parque view, partitioned by name and artist_id \\n\")\n",
    "    spark.sql(\"\"\"\n",
    "              SELECT *\n",
    "              FROM parquetArtistFile\n",
    "              ORDER BY artist_id\n",
    "              LIMIT 5\"\"\").show()\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    # Here for test read local json files only\n",
    "    log_data = input_data\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter whole read df by page= NextSong\n",
    "    df = df.filter(df['page']=='NextSong')\n",
    "    \n",
    "    # drop any ele with at least NA, without dropna(), there are a lot of rows with null will be returned\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(\"+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####\\n\")\n",
    "    print(\"to see the original df schema of log data \\n\")\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    #df = \n",
    "    \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    print(\"extract from root schema to be  dimensional table songs_table \\n\")\n",
    "    users_table = df.select(col('userId').alias('user_id'), \\\n",
    "                            col('firstName').alias('first_name'), \\\n",
    "                            col('lastName').alias('last_name'), \\\n",
    "                            'gender', \\\n",
    "                            'level')\n",
    "\n",
    "    users_table.printSchema()\n",
    "    users_table.show(5)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write \\\n",
    "               .partitionBy(\"user_id\") \\\n",
    "               .mode('overwrite') \\\n",
    "               .parquet(\"users.parquet\")\n",
    "\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "     # read back parquet files after saving above\n",
    "    parquetUsersFile = spark.read.parquet(\"users.parquet\")\n",
    "    \n",
    "    # creat tempo parquet view\n",
    "    print(\"<###################################################################### \\n\")\n",
    "    parquetUsersFile.createOrReplaceTempView(\"parquetUsersFile\")\n",
    "\n",
    "    print(\"to see users parque view, partitioned by name and user_id \\n\")\n",
    "    spark.sql(\"\"\"\n",
    "              SELECT *\n",
    "              FROM parquetUsersFile\n",
    "              ORDER BY user_id\n",
    "              LIMIT 5\"\"\").show()\n",
    "    \n",
    "   # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # raw table provide ts as epoch timestamps, need to convert to datetime_type (supported by spark)\n",
    "    # create timestamp column from original timestamp column with interval 1s as start_time\n",
    "    \n",
    "    get_timestamp = udf(lambda x:  datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # extract the column start_time\n",
    "    df = df.withColumn(\"start_time\", get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d'))\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df.ts))\n",
    "    \n",
    "    # remaining columns of timetable will be used by `pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format`\n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('start_time', \\\n",
    "                              hour(col(\"start_time\")).alias('hour'), \\\n",
    "                             date_format(col(\"datetime\"), 'd').alias('day'), \\\n",
    "                             weekofyear(col(\"datetime\")).alias('week'), \\\n",
    "                             month(col(\"datetime\")).alias('month'), \\\n",
    "                             year(col(\"datetime\")).alias('year'), \\\n",
    "                              weekofyear(col(\"datetime\")).alias('weekday')\n",
    "                          )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write \\\n",
    "              .mode('overwrite') \\\n",
    "              .partitionBy('year', 'month') \\\n",
    "              .parquet(\"time_table.parquet\")\n",
    "    \n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # Just for see new time_table schema\n",
    "    # read back parquet files after saving above\n",
    "#     parquetTimeTableFile = spark.read.parquet(\"time_table.parquet\")\n",
    "    \n",
    "#     print(\"<###################################################################### \\n\")\n",
    "#     parquetTimeTableFile.createOrReplaceTempView(\"parquetTimeTableFile\")\n",
    "\n",
    "#     print(\"to see timetable parque view, partitioned by `year` and `month` \\n\")\n",
    "#     spark.sql(\"\"\"\n",
    "#               SELECT *\n",
    "#               FROM parquetTimeTableFile\n",
    "#               ORDER BY start_time\n",
    "#               LIMIT 5\"\"\").show()\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    ####################################################################################\n",
    "    # read in song data to use for songplays table\n",
    "    input_data_song_clone = 'data/song_data/A/A/A/*.json'\n",
    "    song_df = spark.read.json(input_data_song_clone)\n",
    "    \n",
    "    #test see song_data inside log_data process\n",
    "    print(\"see song_data gain inside log_data process \\n\")\n",
    "    song_df.printSchema()\n",
    "    song_df.show(3)\n",
    "\n",
    "\n",
    "#     df.createOrReplaceTempView(\"log_df\")\n",
    "#     log_df_table = spark.sql(''' SELECT DISTINCT datetime, song, artist, userId, level, sessionId, location, userAgent\n",
    "#                                  FROM log_df\n",
    "#                              ''')\n",
    "#     print(\"to see Schema of log_df_table view only.......................... \\n\")\n",
    "#     log_df_table.printSchema()\n",
    "#     log_df_table.show(2)\n",
    "    \n",
    "#     # creat view with same name as datafram song_df\n",
    "#     song_df.createOrReplaceTempView(\"song_df\")\n",
    "#     song_df_table = spark.sql(''' SELECT DISTINCT song_id, title, artist_id, artist_name\n",
    "#                                   FROM song_df\n",
    "#                               ''')\n",
    "    \n",
    "#     ##DEUG ONLY\n",
    "#     print(\"to see Schema of song_df_table view only.......................... \\n\")\n",
    "#     song_df_table.printSchema()\n",
    "#     song_df_table.show(2)\n",
    "#     ##DEUG ONLY\n",
    "#     print(\"========================================================= \\n\")\n",
    "    \n",
    "    # use pyspark.sql.functions.row_number() to make songplay_id\n",
    "    # extract columns from joined song and log datasets to create songplays table\n",
    "    \n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    # just for easy debug marking log_df is log dataframe     \n",
    "    log_df = df\n",
    "\n",
    "    #two_table = log_df_table.join(song_df_table, on = (log_df_table.song == song_df_table.title) & (log_df_table.artist == song_df_table.artist_name), how = 'left_outer')\n",
    "    two_table = log_df.join(song_df, \\\n",
    "                            (log_df.song == song_df.title) \\\n",
    "                            & (log_df.artist == song_df.artist_name) \\\n",
    "                            & (log_df.length == song_df.duration), \\\n",
    "                            'left_outer')\n",
    "    \n",
    "    ## DEUG ONLY ##############################\n",
    "#     print(\"to see Schema of two_table only.......................... \\n\")\n",
    "#     two_table.printSchema()\n",
    "#     two_table.show(1)\n",
    "    ## DEUG ONLY ##############################\n",
    "\n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#     # Add songplay_id by monotonically_increasing_id() method\n",
    "    two_table = two_table.withColumn('songplay_id', monotonically_increasing_id())\n",
    "    \n",
    "#      ##DEUG ONLY\n",
    "#     print(\"to see Schema of two_table only.......................... \\n\")\n",
    "#     two_table.printSchema()\n",
    "#     two_table.show(3)\n",
    "#     ##DEUG ONLY\n",
    "#     #>-------------------------------------------------\n",
    "    \n",
    "#     #Creat songplays_table\n",
    "#     # alias made inside df or song_df, is not exist yet\n",
    "    songplays_table = two_table.select('songplay_id', \\\n",
    "                                        col(\"datetime\").alias('start_time'), \\\n",
    "                                        col(\"userId\").alias('user_id'), \\\n",
    "                                        col(\"level\"), \\\n",
    "                                        song_df.song_id, \\\n",
    "                                        song_df.artist_id, \\\n",
    "                                        col(\"sessionId\").alias('session_id'), \\\n",
    "                                        col(\"location\"), \\\n",
    "                                        col(\"userAgent\").alias('user_agent'))\n",
    "#     ## DEUG ONLY ##############################\n",
    "#     print(\"to see Schema of songplays_table only, without year month \\n\")\n",
    "#     songplays_table.printSchema()\n",
    "#     songplays_table.show(3)\n",
    "#     ## DEUG ONLY ##############################\n",
    "\n",
    "#     # Add more `year` , `month` prior- to creat parquet files\n",
    "# #     songplays_table = songplays_table.withColumn('year',year('start_time')) \\\n",
    "# #                                      .withColumn('month',month('start_time'))\n",
    "    songplays_table = songplays_table.withColumn('year', year('start_time'))\\\n",
    "                                      .withColumn('month', month('start_time'))\n",
    "    \n",
    "#     ## DEUG ONLY ##############################\n",
    "#     print(\"to see Schema of songplays_table only, with year month \\n\")\n",
    "#     songplays_table.printSchema()\n",
    "#     songplays_table.show(3)\n",
    "#     ## DEUG ONLY ##############################\n",
    "    \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write\\\n",
    "                   .mode('overwrite')\\\n",
    "                   .partitionBy('year', 'month')\\\n",
    "                   .parquet(\"songplays.parquet\")\n",
    "\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # read back parquet files after saving above\n",
    "    parquetSongPlaysFile = spark.read.parquet(\"songplays.parquet\")\n",
    "\n",
    "    print(\"<###################################################################### \\n\")\n",
    "    \n",
    "    #creat tempo songsplay parquet view\n",
    "    parquetSongPlaysFile.createOrReplaceTempView(\"parquetSongPlaysFile\")\n",
    "\n",
    "    print(\"to see songsplay parque view, partitioned by `year` and `month` \\n\")\n",
    "    spark.sql(\"\"\"\n",
    "              SELECT *\n",
    "              FROM parquetSongPlaysFile\n",
    "              ORDER BY month\n",
    "              LIMIT 5\"\"\").show()\n",
    "    # TO REVIEW ONLY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # input_data = \"s3a://udacity-dend/\"\n",
    "\n",
    "    # Test with small dataset in workspace\n",
    "#     input_data_song = 'data/song-data.zip'\n",
    "#     input_data_log = 'data/log-data.zip'\n",
    "    input_data_song = 'data/song_data/A/A/A/*.json'\n",
    "    input_data_log = 'data/log_data/*.json'\n",
    "\n",
    "    output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, input_data_song, output_data)\n",
    "\n",
    "    process_log_data(spark, input_data_log, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to see the original df read from json song data \n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARKFYS91187B98E58F|           null|                    |            null|Jeff And Sheri Ea...| 267.7024|        1|SOYMRWW12A6D4FAB14|The Moon And I (O...|   0|\n",
      "|AR10USD1187B99F3F1|           null|Burlington, Ontar...|            null|Tweeterfriendly M...|189.57016|        1|SOHKNRJ12A6701D1F8|        Drop of Rain|   0|\n",
      "|ARGSJW91187B9B1D6B|       35.21962|      North Carolina|       -80.01955|        JennyAnyKind|218.77506|        1|SOQHXMF12AB0182363|     Young Boy Blues|   0|\n",
      "|ARMJAGH1187FB546F3|       35.14968|         Memphis, TN|       -90.04892|        The Box Tops|148.03546|        1|SOCIWDW12A8C13D406|           Soul Deep|1969|\n",
      "|AR7G5I41187FB4CE6C|           null|     London, England|            null|            Adam Ant|233.40363|        1|SONHOTT12A8C13493C|     Something Girls|1982|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "extract from root schema to be  dimensional table songs_table \n",
      "\n",
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F|   0| 267.7024|\n",
      "|SOHKNRJ12A6701D1F8|        Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|\n",
      "|SOQHXMF12AB0182363|     Young Boy Blues|ARGSJW91187B9B1D6B|   0|218.77506|\n",
      "|SOCIWDW12A8C13D406|           Soul Deep|ARMJAGH1187FB546F3|1969|148.03546|\n",
      "|SONHOTT12A8C13493C|     Something Girls|AR7G5I41187FB4CE6C|1982|233.40363|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "to see songs parque view, partitioned by artist_id and year \n",
      "\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...| 267.7024|   0|ARKFYS91187B98E58F|\n",
      "|SOUDSGM12AC9618304|Insatiable (Instr...|266.39628|   0|ARNTLGG11E2835DDB9|\n",
      "|SOMJBYD12A6D4F8557|Keepin It Real (S...|114.78159|   0|ARD0S291187B9B7BF5|\n",
      "|SOMZWCG12A8C13C480|    I Didn't Mean To|218.93179|   0|ARD7TVE1187B99BFB1|\n",
      "|SOQHXMF12AB0182363|     Young Boy Blues|218.77506|   0|ARGSJW91187B9B1D6B|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "\n",
      "<###################################################################### \n",
      "\n",
      "extract from root schema to be dimensional table artists_table \n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "|         artist_id|                name|            location|latitude|longitude|\n",
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "|ARKFYS91187B98E58F|Jeff And Sheri Ea...|                    |    null|     null|\n",
      "|AR10USD1187B99F3F1|Tweeterfriendly M...|Burlington, Ontar...|    null|     null|\n",
      "|ARGSJW91187B9B1D6B|        JennyAnyKind|      North Carolina|35.21962|-80.01955|\n",
      "|ARMJAGH1187FB546F3|        The Box Tops|         Memphis, TN|35.14968|-90.04892|\n",
      "|AR7G5I41187FB4CE6C|            Adam Ant|     London, England|    null|     null|\n",
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "######################################################################### \n",
      " \n",
      "to see artists parque view, partitioned by name and artist_id \n",
      "\n",
      "+--------------------+--------+---------+--------------------+------------------+\n",
      "|            location|latitude|longitude|                name|         artist_id|\n",
      "+--------------------+--------+---------+--------------------+------------------+\n",
      "|Burlington, Ontar...|    null|     null|Tweeterfriendly M...|AR10USD1187B99F3F1|\n",
      "|     London, England|    null|     null|            Adam Ant|AR7G5I41187FB4CE6C|\n",
      "|                    |    null|     null|    Planet P Project|AR8ZCNI1187B9A069B|\n",
      "|                Ohio|    null|     null|             Rated R|ARD0S291187B9B7BF5|\n",
      "|     California - LA|    null|     null|              Casual|ARD7TVE1187B99BFB1|\n",
      "+--------------------+--------+---------+--------------------+------------------+\n",
      "\n",
      "+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####+++++####\n",
      "\n",
      "to see the original df schema of log data \n",
      "\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|     The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|            Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|\n",
      "|Sony Wonder|Logged In|   Samuel|     M|            0|Gonzalez|218.06975| free|Houston-The Woodl...|   PUT|NextSong|1.540492941796E12|      597|           Blackbird|   200|1542253449796|\"Mozilla/5.0 (Mac...|    61|\n",
      "|  Van Halen|Logged In|    Tegan|     F|            2|  Levine|289.38404| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Best Of Both Worl...|   200|1542260935796|\"Mozilla/5.0 (Mac...|    80|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "extract from root schema to be  dimensional table songs_table \n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     61|    Samuel| Gonzalez|     M| free|\n",
      "|     80|     Tegan|   Levine|     F| paid|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "<###################################################################### \n",
      "\n",
      "to see users parque view, partitioned by name and user_id \n",
      "\n",
      "+----------+---------+------+-----+-------+\n",
      "|first_name|last_name|gender|level|user_id|\n",
      "+----------+---------+------+-----+-------+\n",
      "|   Jizelle| Benjamin|     F| free|      2|\n",
      "|   Jizelle| Benjamin|     F| free|      2|\n",
      "|   Jizelle| Benjamin|     F| free|      2|\n",
      "|   Jizelle| Benjamin|     F| free|      2|\n",
      "|   Jizelle| Benjamin|     F| free|      2|\n",
      "+----------+---------+------+-----+-------+\n",
      "\n",
      "see song_data gain inside log_data process \n",
      "\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARKFYS91187B98E58F|           null|                    |            null|Jeff And Sheri Ea...| 267.7024|        1|SOYMRWW12A6D4FAB14|The Moon And I (O...|   0|\n",
      "|AR10USD1187B99F3F1|           null|Burlington, Ontar...|            null|Tweeterfriendly M...|189.57016|        1|SOHKNRJ12A6701D1F8|        Drop of Rain|   0|\n",
      "|ARGSJW91187B9B1D6B|       35.21962|      North Carolina|       -80.01955|        JennyAnyKind|218.77506|        1|SOQHXMF12AB0182363|     Young Boy Blues|   0|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "<###################################################################### \n",
      "\n",
      "to see songsplay parque view, partitioned by `year` and `month` \n",
      "\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|songplay_id|start_time|user_id|level|song_id|artist_id|session_id|            location|          user_agent|year|month|\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|          0|2018-11-15|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          1|2018-11-15|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          2|2018-11-15|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          3|2018-11-15|     61| free|   null|     null|       597|Houston-The Woodl...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          4|2018-11-15|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
